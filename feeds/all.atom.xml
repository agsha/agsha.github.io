<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>A Pelican Blog</title><link href="/" rel="alternate"></link><link href="/feeds/all.atom.xml" rel="self"></link><id>/</id><updated>2020-08-31T12:22:00+00:00</updated><entry><title>Cookbook</title><link href="/cookbook.html" rel="alternate"></link><published>2020-08-31T12:22:00+00:00</published><updated>2020-08-31T12:22:00+00:00</updated><author><name>Sharath Gururaj</name></author><id>tag:None,2020-08-31:/cookbook.html</id><summary type="html">&lt;p&gt;This article captures cookbooks or recipes that I don't use frequently enough to remember, but still use it enough to be painful to google from scratch every time I use it. Its a bunch of totally unrelated stuff that is too short to put as a blog article on its …&lt;/p&gt;</summary><content type="html">&lt;p&gt;This article captures cookbooks or recipes that I don't use frequently enough to remember, but still use it enough to be painful to google from scratch every time I use it. Its a bunch of totally unrelated stuff that is too short to put as a blog article on its own. Hopefully, you'll find the table of contexts helpful to navigate.
(the links are not working yet :( )&lt;/p&gt;
&lt;h1&gt;Table of contents&lt;/h1&gt;
&lt;!-- toc --&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="#julia-arrays"&gt;Julia arrays&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#anaconda-dont-start-at-shell"&gt;Anaconda dont start at shell&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#how-to-mount-samba-folder-in-ubuntu"&gt;How to mount samba folder in ubuntu&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#how-to-use-du-to-search-for-hidden-folders"&gt;How to use du to search for hidden folders&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#sublime-text-keyboard-shortcuts"&gt;Sublime text keyboard shortcuts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#making-nomachine-nx-work-on-amazon-aws-ec2"&gt;Making Nomachine nx work on amazon aws ec2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#remote-desktop-to-raspberry-pi"&gt;Remote desktop to raspberry pi&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#echo-server-with-netcat"&gt;Echo server with netcat&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#mounting-a-disk-on-gcp"&gt;Mounting a disk on GCP&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#describe-type-of-command"&gt;Describe type of command&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#linux-networking"&gt;Linux Networking&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#postgres"&gt;Postgres&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#intellij-shortcuts"&gt;Intellij Shortcuts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#bash-shortcuts"&gt;Bash shortcuts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#how-to-setup-a-new-linux-machine"&gt;How to setup a new linux machine&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#python-regex-recipes"&gt;Python regex recipes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#vim-one-liners-and-shortcuts"&gt;Vim one liners and shortcuts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#cscope-for-linux"&gt;Cscope for linux&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#cscope-for-ceph"&gt;Cscope for ceph&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#building-linux"&gt;Building linux&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#using-ftrace-to-trace-linux-functions"&gt;Using Ftrace to trace linux functions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#how-to-build-linux-on-one-machine-and-deploy-on-another"&gt;How to build linux on one machine and deploy on another&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#jemalloc"&gt;JeMalloc&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#objdump"&gt;Objdump&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#installing-latest-version-of-cmake"&gt;Installing latest version of cmake&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#getting-honest-profiler-to-work"&gt;Getting honest profiler to work&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#gcc-important-options"&gt;GCC important options&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#when-creating-a-shared-library"&gt;When creating a shared library:&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#eli-bendersky-static-linking-summary"&gt;Eli bendersky static linking summary&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#eli-bendersky-load-time-linking-summary"&gt;Eli bendersky load time linking summary&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#eli-bendersky-x32-pic-linking-summary"&gt;Eli bendersky x32 PIC linking summary&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#linux-interrupt-handling"&gt;Linux Interrupt Handling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#hdfs-proxy-user-setting"&gt;HDFS proxy user setting&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- tocstop --&gt;

&lt;h1&gt;&lt;a name="julia-arrays"&gt;Julia Arrays&lt;/a&gt;&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;tabs or spaces: concat to the right&lt;/li&gt;
&lt;li&gt;semicolon or newline: concat to the bottom &lt;/li&gt;
&lt;li&gt;one dimensional arrays are considered as column vectors for concatenation purposes&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;Anaconda dont start at shell&lt;/h1&gt;
&lt;p&gt;If you'd prefer that conda's base environment not be activated on startup, &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;conda config --set auto_activate_base &lt;span class="nb"&gt;false&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h1&gt;How to mount samba folder in ubuntu&lt;/h1&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;sudo mount -t cifs -o &lt;span class="nv"&gt;user&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;sharath //192.168.29.28/HOMEPI /mnt/pi
&lt;/pre&gt;&lt;/div&gt;


&lt;h1&gt;How to use du to search for hidden folders&lt;/h1&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;du -schx .&lt;span class="o"&gt;[&lt;/span&gt;!.&lt;span class="o"&gt;]&lt;/span&gt;* *
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;To exclude other mount points use &lt;code&gt;--exclude&lt;/code&gt;&lt;/p&gt;
&lt;h1&gt;Sublime text keyboard shortcuts&lt;/h1&gt;
&lt;p&gt;Open any file &lt;code&gt;ctrl-p&lt;/code&gt;
Open any command &lt;code&gt;ctrl-shift-p&lt;/code&gt;
Close tab: mac &lt;code&gt;cmd+w&lt;/code&gt;
Linux &lt;code&gt;ctrl-shift-w&lt;/code&gt;
Jump back &lt;code&gt;alt -&lt;/code&gt; 
forward &lt;code&gt;alt +&lt;/code&gt; &lt;/p&gt;
&lt;h1&gt;Making Nomachine nx work on amazon aws ec2&lt;/h1&gt;
&lt;p&gt;First, download the nx package from nomachine and &lt;code&gt;dpkg -i&lt;/code&gt; the package
The software is installed in &lt;code&gt;/usr/NX&lt;/code&gt;
We have to first disable password login and only use certificate login. To do this, edit the file &lt;code&gt;vim /usr/NX/etc/server.config&lt;/code&gt; and add this line. After this, restart the nx server &lt;code&gt;/usr/NX/bin/nxserver --restart&lt;/code&gt;
Verify that password login does not work
For resolution, the key idea is that there should be no x server running already on the machine. If there is no x server, then nx will create its own, and by default try to match the client resolution, which is what we want. Unfortunately, when we install gnome desktop, it automatically changes the systemd runlevel to graphical.target which means systemd will spawn gdm and an xserver /usr/lib/xorg/Xorg (hope you dont get into wayland and shit). So we have to change the default systemd target to shell mode, which can be done by sudo systemctl set-default multi-user.target
To change the current target without restarting, you can do sudo systemctl set-default multi-user.target 
And thats it, nx should automatically try to match client resolution if client is running in full screen mode!&lt;/p&gt;
&lt;p&gt;Sometimes, you will see a black screen when you login, to solve this&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;sudo service gdm stop
/usr/NX/bin/nxserver --restart
&lt;/pre&gt;&lt;/div&gt;


&lt;h1&gt;Remote desktop to raspberry pi&lt;/h1&gt;
&lt;p&gt;We will use real vnc. From terminal, ssh to raspberry pi. On the pi run
&lt;code&gt;vncserver -geometry 1920x1080&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;From the client, run vncviewer from real vnc and login. You have to do this every time you want to login&lt;/p&gt;
&lt;h1&gt;Echo server with netcat&lt;/h1&gt;
&lt;p&gt;Server
&lt;code&gt;ncat -e /bin/cat -k  -l 8888 &amp;lt;ip&amp;gt;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Client 
&lt;code&gt;telnet ip 8888&lt;/code&gt;&lt;/p&gt;
&lt;h1&gt;Mounting a disk on GCP&lt;/h1&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;sudo lsblk
sudo mkdir -p /mnt/disks/sdb
sudo cp /etc/fstab /etc/fstab.backup
sudo blkid /dev/sdb
In /etc/fstb
&lt;span class="nv"&gt;UUID&lt;/span&gt;&lt;span class="o"&gt;=[&lt;/span&gt;UUID_VALUE&lt;span class="o"&gt;]&lt;/span&gt; /mnt/disks/&lt;span class="o"&gt;[&lt;/span&gt;MNT_DIR&lt;span class="o"&gt;]&lt;/span&gt; ext4 discard,defaults,nofail &lt;span class="m"&gt;0&lt;/span&gt; &lt;span class="m"&gt;2&lt;/span&gt;
Sudo mount -a 
&lt;/pre&gt;&lt;/div&gt;


&lt;h1&gt;Describe type of command&lt;/h1&gt;
&lt;p&gt;&lt;code&gt;type ls&lt;/code&gt;&lt;/p&gt;
&lt;h1&gt;Linux Networking&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Netfilter&lt;/strong&gt;: linux component which has various hooks for network packet manipulation
The netfilter hooks are exposed through various  userspace programs: notably &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Iptables&lt;/strong&gt; : which is the userspace interface to linux firewall, (firewall here simply means configuring netfilter according to some rules), nat mangling etc &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Subnetting/CIDR&lt;/strong&gt; : CIDR splits 32 bit ipv4 addresses into two parts: left part is network part. Right part is host part. This act is called subnetting. &lt;code&gt;0.0.0.0/24&lt;/code&gt; Here 24 means, the network part (left part) is 24 bits. The host part (right part is 32-24 = 8) bits. Usually in large companies different networks are indeed different networks. I.e., we need routing between two subnets. With a single (sub)network, there is no need for routing (works like lan)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Network interfaces&lt;/strong&gt;: on the linux level, they are network devices, with a device driver. They get a packet trhough hard_start_xmit() and they can do whatever they want with it. See this for a good example&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Bridge&lt;/strong&gt;: when referring to a hardware device it is a connector at L2. i.e., it learns mac addresses and forwards packets from one network to another switch is also essentially the same thing&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Linux software bridge device&lt;/strong&gt;:  is a software network_interface which acts like a bridge between two other network interfaces&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;Postgres&lt;/h1&gt;
&lt;p&gt;To create a data directory, first &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;mkdir -p /path/to/data
chown postgres  /path/to/data
Sudo -u postgres /usr/lib/postgresql/11/bin/Initdb -D /path/to/data
vim /etc/postgresql/11/main/postgresql.conf &lt;span class="c1"&gt;#and modify the data_dir&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;To make postgres listen to remote connections in the same file:
&lt;code&gt;listen_addresses = '*'&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;vim /etc/postgresql/11/main/pg_hba.conf&lt;/code&gt;
Add this line (probably give stronger restrictions)&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;host    all             all             0.0.0.0/0            md5
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Restart postgres. There are several ways. One of these should work:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    &lt;span class="n"&gt;Sudo&lt;/span&gt; &lt;span class="n"&gt;service&lt;/span&gt; &lt;span class="n"&gt;postgresql&lt;/span&gt; &lt;span class="n"&gt;restart&lt;/span&gt;
&lt;span class="n"&gt;Or&lt;/span&gt; 
    &lt;span class="n"&gt;pg_ctlcluster&lt;/span&gt; &lt;span class="mi"&gt;12&lt;/span&gt; &lt;span class="n"&gt;main&lt;/span&gt; &lt;span class="n"&gt;start&lt;/span&gt;
&lt;span class="n"&gt;Or&lt;/span&gt; 
&lt;span class="n"&gt;pg_ctl&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;usr&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;local&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;var&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;postgres&lt;/span&gt; &lt;span class="n"&gt;start&lt;/span&gt;
&lt;span class="n"&gt;Or&lt;/span&gt;
    &lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;usr&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;lib&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;postgresql&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;bin&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;pg_ctl&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;etc&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;postgresql&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;main&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;l&lt;/span&gt; &lt;span class="n"&gt;logfile&lt;/span&gt; &lt;span class="n"&gt;start&lt;/span&gt;
&lt;span class="n"&gt;Or&lt;/span&gt;
   &lt;span class="n"&gt;sudo&lt;/span&gt; &lt;span class="n"&gt;systemctl&lt;/span&gt; &lt;span class="n"&gt;start&lt;/span&gt; &lt;span class="n"&gt;postgresql&lt;/span&gt;&lt;span class="mi"&gt;@10&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;main&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Create users by first logging in &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;sudo -u postgres psql
create user sharath with encrypted password &lt;span class="s1"&gt;&amp;#39;foo&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
ALTER USER sharath WITH SUPERUSER&lt;span class="p"&gt;;&lt;/span&gt;
create database sharath&lt;span class="p"&gt;;&lt;/span&gt;

&lt;span class="c1"&gt;#To enable the logs in /etc/postgresql/11/main/postgresql.conf&lt;/span&gt;
&lt;span class="nv"&gt;log_destination&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;csvlog&amp;#39;&lt;/span&gt;
&lt;span class="nv"&gt;logging_collector&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; on
&lt;span class="nv"&gt;log_rotation_age&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; 0d
&lt;span class="nv"&gt;log_statement&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;all&amp;#39;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;To view the logs
&lt;code&gt;sudo grep SQLEditor /var/lib/postgresql/12/main/log/* |grep -v pg_catalog | grep -v SHOW | grep -v current_schema&lt;/code&gt;&lt;/p&gt;
&lt;h1&gt;Intellij Shortcuts&lt;/h1&gt;
&lt;p&gt;Ctrl+shift+enter to complete statement after code completion. Its pretty magical. 
Alt+j to select all instances
Tab after ctrl-space overwrites rather than insert
Ctrl+alt-l to reformat whole file or selections
Ctrl+shift+i to see the definition (code) for the method
F2 to go to next highlighted error
F4 go to definition
Ctrl-f12 to see the file structure
Ctrl+shift+numpad- collapse all&lt;/p&gt;
&lt;h1&gt;Bash shortcuts&lt;/h1&gt;
&lt;p&gt;Alt-B back one word
Alt-f forward one word
Ctrl-w delete back one word
alt-D delete till end of word
Ctrl-U delete to start of line
Ctrl-k delete to end of line&lt;/p&gt;
&lt;h1&gt;How to setup a new linux machine&lt;/h1&gt;
&lt;p&gt;First, login as root. Here nm&amp;gt; means new machine and local&amp;gt; means laptop&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;nm&amp;gt; 
adduser -u &lt;span class="m"&gt;1001&lt;/span&gt; sharath&lt;span class="p"&gt;;&lt;/span&gt;
usermod -aG sudo sharath
su - sharath
ssh-keygen -t rsa -N &lt;span class="s2"&gt;&amp;quot;&amp;quot;&lt;/span&gt; -f ~/.ssh/id_rsa
&amp;lt;create a new key&amp;gt;
&amp;lt;copy your localhost id_rsa.pub into clipboard&amp;gt;
vim /home/sharath/.ssh/authorized_keys 
&amp;lt;paste your id_rsa.pub and &lt;span class="nb"&gt;exit&lt;/span&gt; vim&amp;gt;
chmod &lt;span class="m"&gt;644&lt;/span&gt; /home/sharath/.ssh/authorized_keys
&lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;sharath ALL=(ALL) NOPASSWD: ALL&amp;#39;&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt; sudo tee /etc/sudoers.d/sharath
sudo chmod &lt;span class="m"&gt;0440&lt;/span&gt; /etc/sudoers.d/sharath
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;IMPROVED (scriptable)&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;First, copy your public key to tmp on nm, like so
scp ~/.ssh/id_rsa.pub root@&amp;lt;new_ip&amp;gt;:/tmp/ak 

Login to the new box as root and &lt;span class="k"&gt;do&lt;/span&gt; the following
adduser --disabled-password --gecos ‘’ sharath
usermod -aG sudo sharath
su - sharath
ssh-keygen -t rsa -N &lt;span class="s2"&gt;&amp;quot;&amp;quot;&lt;/span&gt; -f ~/.ssh/id_rsa
&lt;span class="nb"&gt;exit&lt;/span&gt;
cp /tmp/ak /home/sharath/.ssh/authorized_keys
chmod &lt;span class="m"&gt;644&lt;/span&gt; /home/sharath/.ssh/authorized_keys
&lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;sharath ALL=(ALL) NOPASSWD: ALL&amp;#39;&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt; sudo tee /etc/sudoers.d/sharath
sudo chmod &lt;span class="m"&gt;0440&lt;/span&gt; /etc/sudoers.d/sharath
rm /tmp/ak
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Bob should be your uncle&lt;/p&gt;
&lt;h1&gt;Python regex recipes&lt;/h1&gt;
&lt;p&gt;\s whitespace
\S non whitespace
\d only digits
\D non digits
\w alphanumeric [a-zA-Z0-9_]
\W non-alphanumeric
\d{1,3}.\d{1,3}.\d{1,3}.\d{1,3} ip regex&lt;/p&gt;
&lt;p&gt;groups enclose in (), numbered from x.group(1)
named groups like this (?P&amp;lt;name) access with x.group("name")&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;re&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;regexStr&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;finditer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;group&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;group&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;r&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;re&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;r&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;(blue|red)&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sub&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;group&lt;/span&gt;&lt;span class="p"&gt;())),&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;blue socks and red shoes&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;prints &lt;code&gt;4 socks and 3 shoes&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;validate a regex on https://regex101.com/&lt;/p&gt;
&lt;h1&gt;Vim one liners and shortcuts&lt;/h1&gt;
&lt;p&gt;gd will go to local definition
gD will go to global definition 
,c&lt;space&gt; toggle comment
&lt;ctrl&gt;]  goes to a tag
&lt;ctrl&gt;o (“old” comes back from a jump)
&lt;ctrl&gt; i (opposite of &lt;ctrl&gt;o
Split window ctrl-w v or o
Scroll up/down, keeping your cursor in its row
One line
Ctrl+Y → Move viewport down
Ctrl+E → Move viewport up (Extra lines)
Column select &lt;ctrl&gt;v 
Go to beginning of function (go to { in first column: needs coding convention) [[ 
End of function ]]
Beginning of code block: [{ this one should work better with messed up coding convention&lt;/p&gt;
&lt;h1&gt;Cscope for linux&lt;/h1&gt;
&lt;p&gt;To apply debian patches use &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;LNX&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;/home/sharath/os/linux/linux-5.4.0
quilt push -a 
find  &lt;span class="nv"&gt;$LNX&lt;/span&gt;  -path &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$LNX&lt;/span&gt;&lt;span class="s2"&gt;/arch/*&amp;quot;&lt;/span&gt; ! -path &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$LNX&lt;/span&gt;&lt;span class="s2"&gt;/arch/x86*&amp;quot;&lt;/span&gt; -prune -o  -path &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$LNX&lt;/span&gt;&lt;span class="s2"&gt;/include/asm-*&amp;quot;&lt;/span&gt; ! -path &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$LNX&lt;/span&gt;&lt;span class="s2"&gt;/include/asm-i386*&amp;quot;&lt;/span&gt; -prune -o  -path &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$LNX&lt;/span&gt;&lt;span class="s2"&gt;/tmp*&amp;quot;&lt;/span&gt; -prune  -path &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$LNX&lt;/span&gt;&lt;span class="s2"&gt;/Documentation*&amp;quot;&lt;/span&gt; -prune -o  -path &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$LNX&lt;/span&gt;&lt;span class="s2"&gt;/scripts*&amp;quot;&lt;/span&gt; -prune -o    -path &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$LNX&lt;/span&gt;&lt;span class="s2"&gt;/drivers*&amp;quot;&lt;/span&gt; -prune -o   -path &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$LNX&lt;/span&gt;&lt;span class="s2"&gt;/.pc/*&amp;quot;&lt;/span&gt; -prune -o  -path &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$LNX&lt;/span&gt;&lt;span class="s2"&gt;/debian/*&amp;quot;&lt;/span&gt; -prune -o   -path &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$LNX&lt;/span&gt;&lt;span class="s2"&gt;/sound/*&amp;quot;&lt;/span&gt; -prune -o    -name &lt;span class="s2"&gt;&amp;quot;*.[chxsS]&amp;quot;&lt;/span&gt; -print &amp;gt; kernel.cscope

cscope -q -R -b -i cscope.files
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;In vim&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;:cs f g &amp;lt;foo&amp;gt; goto definition of foo
:cs f s &amp;lt;foo&amp;gt; find this symbol
:cs f t &amp;lt;foo&amp;gt; find this string
&amp;lt;ctrl&amp;gt;\ s on a word, find this symbol and so on
&lt;/pre&gt;&lt;/div&gt;


&lt;h1&gt;Cscope for ceph&lt;/h1&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;find /mnt/disks/sdb/ceph-pristine -type f -and &lt;span class="se"&gt;\(&lt;/span&gt; -name &lt;span class="s2"&gt;&amp;quot;*.c&amp;quot;&lt;/span&gt; -o -name &lt;span class="s2"&gt;&amp;quot;*.h&amp;quot;&lt;/span&gt; -o -name &lt;span class="s2"&gt;&amp;quot;*.cc&amp;quot;&lt;/span&gt; -o -name &lt;span class="s2"&gt;&amp;quot;*.cpp&amp;quot;&lt;/span&gt; -o -name &lt;span class="s2"&gt;&amp;quot;*.hh&amp;quot;&lt;/span&gt; &lt;span class="se"&gt;\)&lt;/span&gt; &amp;gt; zoo.cscope
cscope -q -R -b -i zoo.cscope 
&lt;/pre&gt;&lt;/div&gt;


&lt;h1&gt;Building linux&lt;/h1&gt;
&lt;p&gt;You can get the linux source from 
&lt;code&gt;apt-get source linux-image-unsigned-$(uname -r)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;You can get the config file from 
&lt;code&gt;cp /boot/config-5.4.0-26-generic .config&lt;/code&gt;
To install dependencies&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;sudo apt-get install libncurses-dev flex bison openssl libssl-dev dkms libelf-dev libudev-dev libpci-dev libiberty-dev autoconf
sudo apt-get build-dep linux linux-image-&lt;span class="k"&gt;$(&lt;/span&gt;uname -r&lt;span class="k"&gt;)&lt;/span&gt;

make &lt;span class="nv"&gt;ARCH&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;x86 &lt;span class="nv"&gt;EXTRAVERSION&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;-SS -j14 &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; make -j14 &lt;span class="nv"&gt;ARCH&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;x86 &lt;span class="nv"&gt;EXTRAVERSION&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;-SS install &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; make -j14 &lt;span class="nv"&gt;ARCH&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;x86 &lt;span class="nv"&gt;EXTRAVERSION&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;-SS modules_install&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;------------------------- DONE --------------------------------&amp;quot;&lt;/span&gt;
rm /boot/initrd.img-3.16.43-SS
update-initramfs -c -k &lt;span class="m"&gt;3&lt;/span&gt;.16.43-SS
update-grub
&lt;/pre&gt;&lt;/div&gt;


&lt;h1&gt;Using Ftrace to trace linux functions&lt;/h1&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt; sudo tee  /sys/kernel/debug/tracing/tracing_on
And &lt;span class="k"&gt;then&lt;/span&gt; use brendan greggs perf-tools
sudo ./funcgraph -D  ext4_readpages &lt;span class="p"&gt;|&lt;/span&gt; sudo tee /mnt/tmpfs/write
sudo ./funcgraph -Dp &lt;span class="m"&gt;51985&lt;/span&gt;  ext4_readpages &lt;span class="p"&gt;|&lt;/span&gt; sudo tee /mnt/tmpfs/write
&lt;/pre&gt;&lt;/div&gt;


&lt;h1&gt;How to build linux on one machine and deploy on another&lt;/h1&gt;
&lt;p&gt;Use “building linux” to build linux on one machine A. Suppose you want to install it on machine B
On machine B&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Mkdir -p ~/kernel/boot
Mkdir -p ~/kernel/modules
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;On machine A&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;tar -cvzf ~/modules /lib/modules/3.16.43-SS
scp -r *SS* &amp;lt;B&amp;gt;:~/kernel/boot
scp ~/modules &amp;lt;B&amp;gt;:~/kernel/modules/
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;On machine B:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Cd ~/kernel/modules
tar -xvzf modules
sudo mv ~/kernel/modules/3.16.43-SS /lib/modules/
sudo cp ~/kernel/boot/* /boot/
sudo rm /boot/initrd.img-3.16.43-SS
sudo update-initramfs -c -k &lt;span class="m"&gt;3&lt;/span&gt;.16.43-SS
sudo update-grub


&lt;span class="c1"&gt;#And… reboot!&lt;/span&gt;
Sudo reboot now
&lt;/pre&gt;&lt;/div&gt;


&lt;h1&gt;JeMalloc&lt;/h1&gt;
&lt;p&gt;Do a git clone.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;./autogen.sh 
./configure --enable-prof --enable-stats --enable-debug
Make


&lt;span class="o"&gt;[&lt;/span&gt;sharath.g@osboxes /home/sharath.g/code/jemalloc&lt;span class="o"&gt;]&lt;/span&gt;$ tree lib 
lib
├── libjemalloc.a
├── libjemalloc_pic.a
├── libjemalloc.so -&amp;gt; libjemalloc.so.2
└── libjemalloc.so.2

&lt;span class="c1"&gt;#To generate the profile:&lt;/span&gt;
&lt;span class="o"&gt;(&lt;/span&gt;reverse-i-search&lt;span class="o"&gt;)&lt;/span&gt;&lt;span class="sb"&gt;`&lt;/span&gt;java&lt;span class="s1"&gt;&amp;#39;: rm jeprof*; rm /tmp/prof.gif; MALLOC_CONF=prof:true,lg_prof_sample:0,lg_prof_interval:25 LD_PRELOAD=${JEMALLOC_PATH}/lib/libjemalloc.so.2 java -Xmx1096m -cp  &amp;#39;&lt;/span&gt;/home/sharath.g/diskbench/:/home/sharath.g/diskbench/*&lt;span class="err"&gt;&amp;#39;&lt;/span&gt; sha.Memory&lt;span class="p"&gt;;&lt;/span&gt; ls jeprof*&lt;span class="p"&gt;;&lt;/span&gt; ~/code/jemalloc/bin/jeprof --show_bytes --gif &lt;span class="sb"&gt;`&lt;/span&gt;which java&lt;span class="sb"&gt;`&lt;/span&gt; jeprof*.heap &amp;gt; /tmp/prof.gif 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;If you’ve compiled jemalloc on some other machine and copied it to this machine, youget teh error Can't exec "objdump": No such file or directory at /home/sharath.g/jemalloc/bin/jeprof line 4459.
Then objdump is 
Jemalloc what is active, dirty (unused), resident, total memory, allocated, mapped?
Allocated = sum of all malloc arguments that has not been freed 
Active .. multiple of page size.. Total bytes in pages that have at least one byte of allocated memory
Resident: active + metadata 
Mapped: multiple of chunk size. Total chunks that have one byte of allocated 
See that man pages for a clear explanation.&lt;/p&gt;
&lt;h1&gt;Objdump&lt;/h1&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;objdump -drSt hello.o
-d dissasemble
-r show relocation info inline
-S show C &lt;span class="nb"&gt;source&lt;/span&gt; code
-t show symbol table
&lt;/pre&gt;&lt;/div&gt;


&lt;h1&gt;Installing latest version of cmake&lt;/h1&gt;
&lt;p&gt;Either apt-get install cmake or get it from here
https://cmake.org/download/
https://cmake.org/files/v3.8/cmake-3.8.0-rc4-Linux-x86_64.sh and run it&lt;/p&gt;
&lt;h1&gt;Getting honest profiler to work&lt;/h1&gt;
&lt;p&gt;Installing it: 
If you’re lucky, you can download it and it works http://insightfullogic.com/honest-profiler.zip&lt;/p&gt;
&lt;p&gt;Otherwise you need to build it. To build it, if you get weird errors, you might need to get latest version of Cmake, and then build it. You need build/libagent.so&lt;/p&gt;
&lt;p&gt;Start the java program with     &lt;br&gt;
&lt;code&gt;-agentpath:$HONEST_PROFILER_HOME/liblagent.so=host=localhost,port=4242,logPath=/grid/1/log/hadoop-yarn/yarn/honest-profiler.hpl&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Start and stop profiling with &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt; &lt;span class="nb"&gt;echo&lt;/span&gt; start &lt;span class="p"&gt;|&lt;/span&gt; nc localhost &lt;span class="m"&gt;4242&lt;/span&gt;
 &lt;span class="nb"&gt;echo&lt;/span&gt; stop &lt;span class="p"&gt;|&lt;/span&gt; nc localhost &lt;span class="m"&gt;4242&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Collapse the stacks with &lt;code&gt;java -cp $HONEST_PROFILER_HOME/honest-profiler.jar com.insightfullogic.honest_profiler.ports.console.FlameGraphDumperApplication netty.hpl netty.cstk&lt;/code&gt;&lt;/p&gt;
&lt;h1&gt;GCC important options&lt;/h1&gt;
&lt;p&gt;The linker &lt;code&gt;ld&lt;/code&gt; takes as input .c files, .o files, .so files and can output an executable or a shared library.
The linker does not output static library. You can simply use ar for it&lt;/p&gt;
&lt;p&gt;Gcc important options
-l mylib 
Look for a library called libmylib.{a, so}
But if both are present, will it choose .a or .so (it chooses .so) if you want to link an .a file, specify it directly on the command line. Or specify -static so it will choose .a even for libc &lt;/p&gt;
&lt;p&gt;Which directories to look in ?
-L dir is the answer&lt;/p&gt;
&lt;p&gt;-i dir is for searching include files 
-include file
Is as if file was #included in source code in the first line. &lt;/p&gt;
&lt;p&gt;So basically &lt;code&gt;-L dir -l mylib -I dir -include files&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;When creating an executable:
You cannot link a shared library statically by giving it on the command line (to create an executable) even if you give on command line, it still links it as  a shared library (as if you had given -L. -lxxx)
Or you cannot link a static library in a shared way by using -L and -l options (to create an executable)(you can but it will be taken as static, even if the static library is named libxx.so) &lt;/p&gt;
&lt;p&gt;Static libraries can be linked (statically) by specifying them as input files to gcc. Remember: static libraries are just a bunch of .o files 
You cannot skip specifying a shared library while linking to create an executable (although, technically they are not needed)  all variables must be resolved, which means that all shared libraries have to be specified (strictly not required, i guess its a sanity check) this has the effect of  listing in the executable as a dependency     &lt;/p&gt;
&lt;p&gt;For &lt;code&gt;LD_PRELOAD&lt;/code&gt; to work, you have to specify -Wl,-soname,libml.so while having created that shared library this name is hardcoded into the so as its name, and this is the name that the linker looks at while loading the executable, not the file name of the .so&lt;/p&gt;
&lt;h1&gt;When creating a shared library:&lt;/h1&gt;
&lt;p&gt;All object files must be compiled with -fpic
All ar files must have .o files which have been compiled with -fpic
If you specify multiple .so on command line, they will be combined into 1 .so as expected, with references resolved internally 
If a dependency is unmet (by not specifying some libraries at all) it is defined as undefined but linking succeeds
If a dependency is met by -L. -lxxx then it is undefined in the .so but that lib is listed as a dependency&lt;/p&gt;
&lt;h1&gt;Eli bendersky static linking summary&lt;/h1&gt;
&lt;p&gt;Static linking algorithm perfectly explained in http://eli.thegreenplace.net/2013/07/09/library-order-in-static-linking
Some important points
* An object file both exports and imports symbols 
* individual object files specified on cmd line are always linked
* static libraries (ar) libraries serve only to provide a list of object files, and an object file in a library is linked only if it satisfies some current unmet dependency &lt;/p&gt;
&lt;h1&gt;Eli bendersky load time linking summary&lt;/h1&gt;
&lt;p&gt;Summary of load time linking as specified in http://eli.thegreenplace.net/2011/08/25/load-time-relocation-of-shared-libraries&lt;/p&gt;
&lt;p&gt;Some gotchas: 
* Call operands are rip relative, but mov operands are absolute. In x86_64, there is a new addressing called RIP relative where any operand can be RIP relative.
* If a shared variable in a .so is referenced from the main executable, then that shared variable will go into the main executable, since the main executable is not relocatable. 
* what about a shared function in a .so referenced from main executable? The article does not mention it, but i imagine the functionality is similar. I.e. put the address of the function in the main program during load time and then call it.&lt;/p&gt;
&lt;p&gt;Basically the address where a shared library is loaded is unknown
The static linker generates .so as if the shared library is loaded at 0x0 (logical)
The linker tells the offset of each variable in the file. 
The loader will add this offset to the load address to get the real address
The linker also populates a list where the real address has to be patched into the code. 
The loader will then patch the addresses in
Thats it.&lt;/p&gt;
&lt;h1&gt;Eli bendersky x32 PIC linking summary&lt;/h1&gt;
&lt;p&gt;http://eli.thegreenplace.net/2011/11/03/position-independent-code-pic-in-shared-libraries&lt;/p&gt;
&lt;p&gt;Instead of direct patching in the code, the real addresses of shared variables are stored in a GOT table in the data section. 
For mov operands, x32 does not support rip relative addresses, so there is a hack to get current rip address, then add the offset of the got table, and thus get the address of the variable&lt;/p&gt;
&lt;p&gt;For call instructions, they are still using the hack but call supports rip relative addressing. So not sure what is happening,&lt;/p&gt;
&lt;h1&gt;Linux Interrupt Handling&lt;/h1&gt;
&lt;p&gt;Peripheral devices pass interrupt to the interrupt controller (8259) which then passes it to processor on a given interrupt line.
An Interrupt line can be shared by multiple devices. To detect which device raised the interrupt, the linux interrupt handler iterates through all registered handlers on that line which then checks if that device raised the interrupt.
When an interrupt is raised, the processor disables all interrupts (on the local processor). 
The linux handler reenables interrupts (unless a flag was set while registering the handler) but disables the interrupt line (so none of the devices sharing can raise another interrupt) and calls the handler. For this reason, an interrupt handler need not be re-entrant. 
An interrupt handler cannot sleep/block because there is no “process” to sleep or block. (theoretically, the process can be considered to be the process that was interrupted but seems like a design decision to not let interrupt handlers sleep or not associate interrupt handlers with a process.&lt;/p&gt;
&lt;h1&gt;HDFS proxy user setting&lt;/h1&gt;
&lt;p&gt;In addition to the modifications in core-site.xml we need to create the user and (add it to a group (?)) on the namenode. Even this is not working. It looks like the user needs to be added on the datanodes too. Is this true?
The logs from namenode are like this: 
2017-06-05 11:48:27,271 INFO  ipc.Server (Server.java:doRead(850)) - Socket Reader #1 for port 8020: readAndProcess from client 10.32.10.3 threw exception [org.apache.hadoop.security.authorize.AuthorizationException: Unauthorized connection for super-user: hue from IP 10.32.10.3]
The ip 10.32.10.3 is a datanode.&lt;/p&gt;</content><category term="misc"></category><category term="misc"></category></entry><entry><title>The quest for high throughput RPC</title><link href="/the-quest-for-high-throughput-rpc.html" rel="alternate"></link><published>2018-07-07T00:30:00+00:00</published><updated>2018-07-07T00:30:00+00:00</updated><author><name>Sharath Gururaj</name></author><id>tag:None,2018-07-07:/the-quest-for-high-throughput-rpc.html</id><summary type="html">&lt;p&gt;In this post, I compare the rpc throughput of some existing RPC frameworks and talk about a new technique of doing RPC . Dropwizard on a single connection gives around &lt;code&gt;1000 req/sec&lt;/code&gt;, whereas a prototype implementation of this technique gives around &lt;code&gt;4 million req/sec&lt;/code&gt; for a “hello world” rpc …&lt;/p&gt;</summary><content type="html">&lt;p&gt;In this post, I compare the rpc throughput of some existing RPC frameworks and talk about a new technique of doing RPC . Dropwizard on a single connection gives around &lt;code&gt;1000 req/sec&lt;/code&gt;, whereas a prototype implementation of this technique gives around &lt;code&gt;4 million req/sec&lt;/code&gt; for a “hello world” rpc&lt;/p&gt;
&lt;p&gt;Lots of companies these days use a microservices architecture, where the application as a whole is split into multiple smaller microservices communicating over the network. What follows in the post is heavy on numbers, which admittedly is boring to parse and makes it a little dry. A nanosec, microsec and a millisec might all just seem like a really small amount of time for humans but the distance between them is vast. If we were to blow up the numbers to human scale and make a nanosec as a second, then it would take one nanosec to pick up a pen next to your laptop, one microsec is a 20 min bike ride, and 1 millisec is a bike ride from bangalore to indore!&lt;/p&gt;
&lt;p&gt;For the time being, I’m going to consider two communicating processes on localhost so we don’t have to worry about network transmission latency for now
Our goal is to exchange 100 byte messages as fast as we can between two processes with a cap on (say 95% percentile) latency (say, &lt;code&gt;&amp;lt;5 ms&lt;/code&gt;)&lt;/p&gt;
&lt;h2&gt;Dropwizard and web services&lt;/h2&gt;
&lt;p&gt;If we hook up a simple dropwizard app with a “hello world” GET api and hit it with apache bench (ab) on localhost laptop with a single connection, I get around &lt;code&gt;1000 req/sec&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;If I pump up the number of concurrent connections, I get around &lt;code&gt;7000 req/s&lt;/code&gt; with around 100 connections (I hit ulimit after that on mac).&lt;/p&gt;
&lt;p&gt;How good or bad is that?&lt;/p&gt;
&lt;p&gt;For an absolute theoretical limit, we can compare it to the rate at which CPUs can read and write streaming data to/from memory. From my measurements, each core can stream data from RAM at around 1GB/sec.
so 1GBps means 10 million req/s with 100 byte messages. When compared to the 800 req/s for a single connection, this is disappointing indeed.&lt;/p&gt;
&lt;p&gt;Of course, dropwizard does a lot more than simply stream data to/from memory. It has to parse http, send the packet down on the tcp stack and do the whole thing back on the rx side.&lt;/p&gt;
&lt;p&gt;So how much time does it take for a packet to traverse the tcp stack? I’ve measured that it takes around ~25 microseconds ballpark to go down or come up the tcp stack. This is also in agreement with some online benchmarks. If we take this into account, it should take around 1/(50 usec) or ~2000 req/sec per core (pair). Compared to this, 800 req/s seems to be a bit too slow, even with the added http parsing.&lt;/p&gt;
&lt;p&gt;With multiple cores and a large number of connections, I was able to crank up dropwizard to around 20K req/sec, but it requires many 100’s of connections to parallelize the processing. But keeping in mind the 10 million req/sec, this is still disappointing&lt;/p&gt;
&lt;p&gt;The problem with using jetty/dropwizard for RPC is that, fundamentally, the architectures these web servers are tuned for entirely different things. Web servers are meant to operate on the internet, where a server can hold many tens of thousands of browser connections, with each browser connection being mostly idle and bursty for a short while. This does not map well to a Data Center environment where a server is talking to only about a dozen other services, and the connections are long lived and required to be very high throughput.&lt;/p&gt;
&lt;p&gt;I believe this is causing a systemic performance problem in software companies. For example, we are all aware that disk and database access is orders of magnitude slower than cpu/network. Why then do we put ~20 dropwizard boxes in front of a mysql DB? Shouldn’t it be the other way round? 1 (or a few for redundancy) dropwizard box fronting multiple sharded mysql instances? This webserver problem leads to a vicious cycle: Each client needs to also employ multiple boxes just to generate the paralellism and connections to load a server.&lt;/p&gt;
&lt;p&gt;The summary so far:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;type&lt;/th&gt;
&lt;th&gt;throughput&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;dropwizard single connection&lt;/td&gt;
&lt;td&gt;1000 req/s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;dropwizard 100 connections&lt;/td&gt;
&lt;td&gt;7000 req/s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;TCP packet round trip&lt;/td&gt;
&lt;td&gt;20k req/s (ballpark)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;CPU streaming from RAM&lt;/td&gt;
&lt;td&gt;10M req/s&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;Unix domain sockets&lt;/h2&gt;
&lt;p&gt;Anyway, to continue with our quest for high throughput, lets skip web servers altogether.&lt;/p&gt;
&lt;p&gt;With Unix domain sockets (which works only on localhost), we can bypass the entire tcp stack and take a much shorter route through the kernel. With UDS and a netty server (the default implementation in specter), we have measured around 10K req/s on a single core. Compare this to 800 req/s for dropwizard and 20K req/s if we were to traverse the TCP stack). With all cores (~10 cores), I measured around 40K req/s. This is much better than web servers. But still slow, considering
* we’re skipping the entire tcp stack
* the 1GBps CPU-memory bandwidth&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;type&lt;/th&gt;
&lt;th&gt;throughput&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;UDS single connection&lt;/td&gt;
&lt;td&gt;10K req/s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;UDS multiple connections&lt;/td&gt;
&lt;td&gt;40K req/s&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;Grpc and HTTP2&lt;/h2&gt;
&lt;p&gt;Another fundamental problem with all the previous frameworks are that they operate in inherently thread-request-response modes. i.e., the client thread sends a request and waits for a response. We would be able to achieve much better results if we operate in streaming mode, where the client is streaming multiple requests and the server is streaming back multiple responses. For this, we use HTTP2. It has two big advantages, the wire format is binary (not textual like HTTP) and it supports request streaming/pipelining&lt;/p&gt;
&lt;p&gt;gRPC is a RPC mechanism using HTTP2. I haven’t explicitly measured the performance but according to the available online bencharks, they are able to do around 300K req/sec across two different machines, (which I assume will fall to ~150K) with both client and server on localhost.&lt;/p&gt;
&lt;p&gt;Newer versions of jetty also support HTTP2. I tested out with a barebones embedded HTTP2 jetty server and I was able to get ~40K req/sec which is comparable to the UDS performance. Although less than gRPC, This is impressive considering that jetty has to parse HTTP as well as traverse the tcp stack, which UDS avoids. (Also remember that UDS is operating in request-response mode, not streaming mode)&lt;/p&gt;
&lt;h2&gt;Raw sockets&lt;/h2&gt;
&lt;p&gt;All this begs the question: what is the fundamental limit for streaming data across a TCP connection? I threw up a small experiment with raw sockets in java on localhost and to my surprise, I was able to get the full 1GBps even with the tcp stack. Then where are we going wrong? why aren’t we able to achieve this with existing frameworks and even with http2?
When I was running the TCP streaming test, I realized that the key to high throughput is batching. The cost of traversing the tcp stack is roughly the same whether we send 1 byte or 1kB or 1MB. This is very apparent in the tests. With small messages, the throughput is approx 10K req/s. As we double the message size, the throughput keeps doubling, until it reaches 1GBps. So it makes sense to introduce a small “linger” time to combine multiple messages and send it across at once. How big is “big” for TCP networks? From the tests, I found that ~ 10Kb payload size is able to saturate the network (and memory) line rates&lt;/p&gt;
&lt;p&gt;None of the RPC frameworks that I know of employ this technique. Actually, Kafka producer employs batching but suffers from other architectural issues which prevents it from achieving full line rate (More on that in a future post). In fact, the trend is in the opposite direction, to reduce message latencies further and send across a message as soon as possible. For example, Aeron claims to reduce latency from 50 microsec to 30 microsec by using UDP. The latency improvement might be super important in finance but presents only a very marginal improvements in throughput.&lt;/p&gt;
&lt;h2&gt;The new and shiny&lt;/h2&gt;
&lt;p&gt;I wrote a client and a server which uses this batching technique over raw sockets. I batch small messages together until they reach a size of 20KB (or a timeout expires) and send big batches (~20kb) over tcp. The results were quite astounding. I was able to reach 3 million req/sec with 100 byte messages and only a single tcp connection and a single core (pair).&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;type&lt;/th&gt;
&lt;th&gt;throughput&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;gRPC&lt;/td&gt;
&lt;td&gt;150K req/s (haven't tested)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Jetty HTTP2 server with async client and req pipelining&lt;/td&gt;
&lt;td&gt;40K req/s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Aeron&lt;/td&gt;
&lt;td&gt;30K req/s&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The code has almost no micro-optimizations (like object cache, zero copy, connection pools, etc) but it demostrates the effectiveness of this architecture.I am a little blown away by how much the throughput increasing with a simple technique. The best news is that the throughput remains virtually unaffected even over a 10Gbps network
If you’re interested in checking out the code and trying it out on localhost or in our DC, here is the link.
https://github.com/agsha/sharpc&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;type&lt;/th&gt;
&lt;th&gt;throughput&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style="color:red"&gt;New Batching RPC single TCP connection&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;3M req/s (haven't tested)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Dropwizard/jetty has a well understood and rock solid programming model, but leaves a lot to be desired with respect to max throughput achievable.&lt;/li&gt;
&lt;li&gt;Enabling HTTP2 is quite trivial for Jetty and can be expected to fetch ~30–50% increase in throughput&lt;/li&gt;
&lt;li&gt;Maybe its time to consider a “true” RPC subsystem in software companies: like grpc or thrift&lt;/li&gt;
&lt;li&gt;A batching algorithm can increase throughput by orders of magnitude compared to even high performance frameworks like grpc
and here’s a cat of all the previous tables&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;type&lt;/th&gt;
&lt;th&gt;throughput&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;dropwizard single connection&lt;/td&gt;
&lt;td&gt;1000 req/s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;dropwizard 100 connections&lt;/td&gt;
&lt;td&gt;7000 req/s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;TCP packet round trip&lt;/td&gt;
&lt;td&gt;20k req/s (ballpark)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;CPU streaming from RAM&lt;/td&gt;
&lt;td&gt;10M req/s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;UDS single connection&lt;/td&gt;
&lt;td&gt;10K req/s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;UDS multiple connections&lt;/td&gt;
&lt;td&gt;40K req/s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;gRPC&lt;/td&gt;
&lt;td&gt;150K req/s (haven't tested)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Jetty HTTP2 server with async client and req pipelining&lt;/td&gt;
&lt;td&gt;40K req/s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Aeron&lt;/td&gt;
&lt;td&gt;30K req/s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style="color:red"&gt;New Batching RPC single TCP connection&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;3M req/s (haven't tested)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;</content><category term="misc"></category><category term="misc"></category></entry><entry><title>Notes about kafka metrics</title><link href="/notes-about-kafka-metrics.html" rel="alternate"></link><published>2016-08-23T00:30:00+00:00</published><updated>2016-08-23T00:30:00+00:00</updated><author><name>Sharath Gururaj</name></author><id>tag:None,2016-08-23:/notes-about-kafka-metrics.html</id><content type="html">&lt;h1&gt;Notes about kafka metrics&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Bytes in&lt;/strong&gt; is number of bytes from external producers only. It does not include, for example, the bytes read by followers during replication. So &lt;code&gt;bytes_in = message_produce_rate * average_message_size&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Bytes out&lt;/strong&gt; includes data sent to follower also.&lt;/p&gt;</content><category term="misc"></category><category term="misc"></category></entry><entry><title>A bangalore software engineer's first trip to Bali</title><link href="/a-bangalore-software-engineers-first-trip-to-bali.html" rel="alternate"></link><published>2016-06-23T00:30:00+00:00</published><updated>2016-06-23T00:30:00+00:00</updated><author><name>Sharath Gururaj</name></author><id>tag:None,2016-06-23:/a-bangalore-software-engineers-first-trip-to-bali.html</id><summary type="html">&lt;p&gt;I have just returned from a 8 day trip to Bali. Here are some tips and info, fresh off my mind. These are points that I wish I knew before my trip to Bali, and the tips that I never came across online or on tripadvisor reviews.&lt;/p&gt;
&lt;h3&gt;Flight&lt;/h3&gt;
&lt;p&gt;If you're …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I have just returned from a 8 day trip to Bali. Here are some tips and info, fresh off my mind. These are points that I wish I knew before my trip to Bali, and the tips that I never came across online or on tripadvisor reviews.&lt;/p&gt;
&lt;h3&gt;Flight&lt;/h3&gt;
&lt;p&gt;If you're leaving from bangalore, then consider going first to kochi or chennai, and then going to Bali. It worked out cheaper for me. Flight from Blore to kochi was ~ INR 1000 and flight from kochi to Bali was ~ INR 15000. A direct flight from Blore to Bali was in comparison around INR 24000. An aside: airport food is annoyingly expensive and not good at all. Strongly recommend &lt;em&gt;packing&lt;/em&gt; enough food to last you till your destination --- including layovers. &lt;/p&gt;
&lt;p&gt;If you're travelling in Air Asia, beware! They're the stingiest aircraft I've ever flown in. They don't even offer water for free, even on international routes. Let alone food. Furthermore, &lt;strong&gt;check-in baggage is not free!&lt;/strong&gt; it cost me around 5000 Rupees extra for one piece of check-in luggage. Oh, yes. If you forget to call and pay for check-in baggage before hand, then the cost is &lt;em&gt;really&lt;/em&gt; exorbitant if you check-in at the airport counter.&lt;/p&gt;
&lt;h3&gt;Visa requirements for indians&lt;/h3&gt;
&lt;p&gt;This was the best part about the Bali trip. &lt;strong&gt;Bali has FREE visa-on-arrival for indians!&lt;/strong&gt; Furthermore, they asked almost NO documents. He simply took one look at my passport and stamped "visa approved for 6 months". Even the immigration wait line was non-existent. The whole immigration process took around 2 minutes. Very pleasant experience.&lt;/p&gt;
&lt;h3&gt;Currency conversion&lt;/h3&gt;
&lt;p&gt;Take USD to Bali, not INR. None of the banks I saw exchanged INR for IDR (Indonesian rupiah). On the other hand, literally every other street has 'money changing' shops which changes USD to IDR (And they are legit). I strongly recommend buying dollars. &lt;/p&gt;
&lt;p&gt;I also had my ICICI debit card that worked pretty seamlessly in Bali. I called up ICICI beforehand that I would be using it in Bali. The card was declined in a couple of shops but accepted at around 90% of the shops. I could also draw IDR from most ATMS. For transaction in shops, ICICI charges 3% + competitive currency conversion. For ATM cash withdrawal, ICICI charges INR 125 + 3% of money withdrawn. Most services in Bali don't accept cards. So whenever you can use the card, use it.&lt;/p&gt;
&lt;p&gt;Something that I did not do, but I should have is that I should have checked with ICICI what happens in the even I lose my debit card and am short of cash. &lt;/p&gt;
&lt;h3&gt;Getting around in Bali&lt;/h3&gt;
&lt;p&gt;Bali is a pretty small place. Most places are within an hour or two driving distance. Definitely get a rental motorbike in Bali. Its super convenient. They charge ~INR 300 per day of rental. Public transportation is almost non existant from what I saw. Although, legally, you need an international driving license to ride a motorbike, It is generally tolerated. Even when caught by the police, the fines are low. Around INR 500. I got caught by the police once: when I was driving on the
wrong direction of a one way. I paid around INR 500 as fine and the police dusted my bike seat and showed me the direction to my destination and helped me along the way. You can see firangs (obviously not Balinese) zipping along in bikes all over Bali. &lt;/p&gt;
&lt;h3&gt;Bargain hard&lt;/h3&gt;
&lt;p&gt;Every service in Bali is overpriced, and it is expected that you bargain. Usually you can get a 35% - 50% reduction in the quoted price. Balinese are very friendly (and sympathetic) towards indians as compared to firangs and it is possible to strike better deals than what firangs can manage. So don't be shy to ask for a discount. &lt;/p&gt;
&lt;h3&gt;Names and the caste system in Bali.&lt;/h3&gt;
&lt;p&gt;There are only four first names in Bali: Ketut, komang, wayan, Made (pronounced Madi). The way it works is:
Let &lt;code&gt;String[] A = {"Ketut", "komang", "wayan", "Made"}&lt;/code&gt; be a string array where the index is 0-based. i.e., &lt;code&gt;A[0]&lt;/code&gt; is &lt;code&gt;"ketut"&lt;/code&gt;. Now if you're the &lt;code&gt;x&lt;/code&gt; &lt;sup&gt;th&lt;/sup&gt; eldest son (where the eldest son is &lt;code&gt;1&lt;/code&gt;, and so on), then your first name is &lt;code&gt;A[(x-1)%4]&lt;/code&gt;. The last name of course is different. Curiously though, people always tell their first name (one of the four said names) when you ask them their name. &lt;/p&gt;
&lt;h3&gt;Telephone SIM card and 3G&lt;/h3&gt;
&lt;p&gt;The very first thing you should do when you land is Bali is: you should get a local SIM card with a 3G data limit of ~2GB.&lt;/p&gt;
&lt;p&gt;3G Coverage in Bali is excellent and google maps is your friend in Bali. Oh yeah. &lt;strong&gt;Definitely carry a extra battery pack (or power pack) for your cell phone&lt;/strong&gt;. Its a life saver. This was easily the most important thing I carried to Bali&lt;/p&gt;
&lt;h3&gt;Places to visit.&lt;/h3&gt;
&lt;p&gt;If this is your first time in Bali, then from an indian point of view, and depending on how long you're planning to visit, this is your order of priority for places to stay. &lt;/p&gt;
&lt;h4&gt;Seminyak&lt;/h4&gt;
&lt;p&gt;Total party area with nice beaches and shopping, also accessible to other places like Uluwatu temple in the south and Tanah lot in the north. (No trip to Bali is complete without a visit to these two). Catch the sunset in both places. Its breathtaking. I did explore three or four beaches but I didnt like most of them. For example, kuta beach doesnt have a lot of 'beach' area (The distances from where the waves end to where the roads begin). Sanur beach totally sucked. It has white sands but
its gravelly and rocky. Very uncomfortable. Beti-balug beach in seminyak has nice sand but not so popular and seems a little ghetto. The only one I liked was double-six beach. Its got everything. Beach chairs, soft sand, lots of restaurants nearby and tons of activities. Perfect for spending an entire day.&lt;/p&gt;
&lt;h4&gt;Ubud&lt;/h4&gt;
&lt;p&gt;Best place to stay is right next to monkey forest. Very cultural place and worth exploring the 'non-beachy' side of Bali. From there you can explore other areas in and near ubud, like tagalalong rice terrace, tegunnen water falls etc.&lt;/p&gt;
&lt;h4&gt;Nusa Penida&lt;/h4&gt;
&lt;p&gt;Little off the main touristy spot (its a separate island). I never visited it but I wish I had. Its got lots of stuff to do according to &lt;a href="https://indonesia.tripcanvas.co/bali/unique-places-in-bali/page/3/"&gt;this link&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;Voltage specs in Bali&lt;/h3&gt;
&lt;p&gt;Bali uses the same kind of round pins that we use in india. Not the flat rectangular pins from US. But one major problem: The socket is "hollow" &lt;a href="https://www.tripadvisor.com/LocationPhotoDirectLink-g1465999-d1539040-i79297910-The_Wangsa_Private_Estate-Tanjung_Benoa_Nusa_Dua_Peninsula_Bali.html"&gt;like this&lt;/a&gt;. So if your charger body is bulky, it won't fit inside the hollowness of the sockets. None of my chargers from india fit in the socket. Luckily every airbnb that I stayed had a Bali adapter
or a Bali charger that allowed me to charge.&lt;/p&gt;
&lt;h3&gt;Cost of the trip.&lt;/h3&gt;
&lt;p&gt;This will of course vary a lot for person to person so your mileage may vary. The numbers are for two people for 8 days.
The costs below are &lt;em&gt;excluding&lt;/em&gt; hotel bookings and flights.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th&gt;Cost&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;transport&lt;/td&gt;
&lt;td&gt;Rs 10,200&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;entertainment&lt;/td&gt;
&lt;td&gt;Rs 25,000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;food&lt;/td&gt;
&lt;td&gt;Rs 16,000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;shopping&lt;/td&gt;
&lt;td&gt;Rs 17,000&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The excrutiangly detailed breakup of costs is &lt;a href="https://docs.google.com/spreadsheets/d/1uVaI5bQ702zNCqu6tZL-GoIzKm4CGlxRkincuRJys5k/edit?usp=sharing"&gt;here&lt;/a&gt;&lt;/p&gt;</content><category term="misc"></category><category term="misc"></category></entry><entry><title>The affair between iostat and fio</title><link href="/the-affair-between-iostat-and-fio.html" rel="alternate"></link><published>2016-06-02T16:20:00+00:00</published><updated>2016-06-02T16:20:00+00:00</updated><author><name>Sharath Gururaj</name></author><id>tag:None,2016-06-02:/the-affair-between-iostat-and-fio.html</id><summary type="html">&lt;p&gt;In the following analysis, I'm going to assume you already know what &lt;code&gt;fio&lt;/code&gt; and &lt;code&gt;iostat&lt;/code&gt; is (I'm too lazy to write it down, and there are better articles out there). This post merely explains the relationship between the numbers&lt;/p&gt;
&lt;h3&gt;fio output&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;io = bw*runt
io = iops*runt*bs
issued(r …&lt;/pre&gt;&lt;/div&gt;</summary><content type="html">&lt;p&gt;In the following analysis, I'm going to assume you already know what &lt;code&gt;fio&lt;/code&gt; and &lt;code&gt;iostat&lt;/code&gt; is (I'm too lazy to write it down, and there are better articles out there). This post merely explains the relationship between the numbers&lt;/p&gt;
&lt;h3&gt;fio output&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;io = bw*runt
io = iops*runt*bs
issued(r) = iops*runt
io = ios*bs
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;iostat output&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;[sharath.g@prod-d42ar-osd-a-00079-423040 /home/sharath.g]$ iostat -x -d 2 /dev/vdb 
Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
vdb               0.00     0.00  179.00    0.00   716.00     0.00     8.00     1.00    5.59    5.59    0.00   5.58  99.80
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;some inferences&lt;/h3&gt;
&lt;p&gt;From the line &lt;code&gt;read : io=78556KB, bw=670322 B/s, iops=163 , runt=120004msec&lt;/code&gt; 
we observe that &lt;code&gt;io = bw * runt&lt;/code&gt;. Makes sense.&lt;/p&gt;
&lt;h3&gt;units&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;bs = bytes&lt;/code&gt;, &lt;code&gt;io = KB&lt;/code&gt; &lt;code&gt;bw=KBps&lt;/code&gt;, &lt;code&gt;runt = ms&lt;/code&gt; &lt;code&gt;r/s=number&lt;/code&gt; &lt;code&gt;avrq-sz=sectors (=512 bytes)&lt;/code&gt;&lt;/p&gt;
&lt;h3&gt;libaio,read,sync=0,direct=0&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;bw = bs*iops&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;io = bs*iops*(runt/1000) / 1024&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;rKB/s = r/s * (avgrq-sz*512) / 1024&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Observe the &lt;code&gt;r/s&lt;/code&gt;, &lt;code&gt;rKB/s&lt;/code&gt; and &lt;code&gt;avrq-sz&lt;/code&gt; graphs. Its very interesting. until block size of 4kb, the &lt;code&gt;iops&lt;/code&gt; value remains constant at 175. After this size, the &lt;code&gt;avrq-sz&lt;/code&gt; starts doubling (the file system minimum block size is 4kb but it can increase that block size upto 0.5 mb when we are trying to read bigger blocks). So after bs=4kb, iops drops because there is some time required to do sequential reads as well. It hits a low at 512kb because reading 512kb is eating into the time for making random reads. Now the &lt;code&gt;avrq-sz&lt;/code&gt; maxes out. Now a single fio read requires more than one 'sequential low level reads' so &lt;code&gt;r/s&lt;/code&gt; now increases steadily. Until the point of 16mb where each fio read equals 32 sequential reads at the block layer. This results in very high &lt;code&gt;r/s&lt;/code&gt; and the corresponding &lt;code&gt;rKB/s&lt;/code&gt; shows sequential-like bandwidths. So far so good! If you see the &lt;code&gt;bw&lt;/code&gt; column, its doubling all the way from 1 byte to 512kb. Until 4kb block size this is caused by simply reading more from the 4k filesystem block that is already read anyway. From 4k to 512k, the doubling is caused due to the doubling of file system block size that is being read. You can see from 4k to 512k block size, the &lt;code&gt;bw&lt;/code&gt; pretty much matches with &lt;code&gt;rKB/s&lt;/code&gt;. But what the heck happened after 1MB block size?!?! &lt;code&gt;bw&lt;/code&gt; has shot upto 1.3 GB/s, while rBK/s continues to be around 70MBps range. The linux page cache is causing this. If you see the &lt;code&gt;rKB/s&lt;/code&gt; column, we are doing around 70MBps. We can read the entire 3GB file in &lt;code&gt;3000/70 = 42&lt;/code&gt; seconds. For the rest 18 seconds, the requests are served through ram and the iops and bw shoots through the roof. Also observe that this is the first time that the file could be read entirely into the page cache. Because, for the previous run with &lt;code&gt;2^19&lt;/code&gt; the value of rKB/s is around 50MBps. In 60 seconds, it just managed to (or missed to ) read &lt;code&gt;50*60sec = 3G&lt;/code&gt; file. So we didnt observe cache effects for the previous run. After 2^20, the bw still keeps increasing, probably because we are filling up the page cache earlier in each run (due to increase in &lt;code&gt;rKB/s&lt;/code&gt;) . Furthermore it seems like fio does a &lt;code&gt;drop-cache&lt;/code&gt; to clear the cache before every run (which is good for us). Now observe the &lt;code&gt;iops&lt;/code&gt; graph. After the spectacular shootup at 2^20, the iops keeps halving. This makes perfect sense because the requests are being served from ram. ram being truly random access, the bandwidth has to remain constant. So if you double the block size, the iops should halve. Sweet! &lt;/p&gt;
&lt;p&gt;Unexplained/TODO:
I tried to explain the &lt;code&gt;r/s&lt;/code&gt; graph in more quantitative terms. If my theory holds, then the V shape from 2^16 to 2^23 is caused by seek_time + sequential_throughput.
i.e. to read a block of &lt;code&gt;x kb&lt;/code&gt;, time taken should be &lt;code&gt;seek_time + sequential_throughput_sec_per_kb*x&lt;/code&gt;
Now you can actually calculate &lt;code&gt;seek_time&lt;/code&gt; and &lt;code&gt;sequential_throughput&lt;/code&gt; with the values at 2^17 and 2^18&lt;/p&gt;
&lt;p&gt;When you extrapolate it to other values, I get &lt;code&gt;2^19 = 85 r/s&lt;/code&gt;, &lt;code&gt;2^20 = 107 r/s&lt;/code&gt;, &lt;code&gt;2^21 = 123 r/s&lt;/code&gt; and &lt;code&gt;2^22 is 133 r/s&lt;/code&gt;. This doesnt quite match up with the observed values in the graph. Maybe some other cache is coming into effect here ?&lt;/p&gt;
&lt;h3&gt;sync,read,sync=0,direct=0&lt;/h3&gt;
&lt;p&gt;This is another interesting graph where we observe quite a different phenomenon. First, let's consider the &lt;code&gt;util&lt;/code&gt; column. We observe that until bs=7, the disk is actually underutilized. This means that until bs=7, the entire goddamn process is bottlenecked on CPU. Indeed, when I checked % cpu utilization through top, the CPU was at humming at 100%. Looking at the iops graph, it is constant until bs=7. We conclude that 900K iops is the theoritical maximum that the linux kernel can perform. So 1.1 microseconds per system call. Useful number to remember! 
Looking at the &lt;code&gt;r/s&lt;/code&gt; graph, it is fairly simple. Until bs=7, values keeps doubling. Why? when we double the fio block size, iops being constant, the kernel can read data twice as fast and will thus issue disk reads twice as fast. After bs=8, the number of &lt;code&gt;r/s&lt;/code&gt; have completely saturated. This is the time when the disk is 100% utilized and cpu/kernel/memory is no longer the bottleneck. The &lt;code&gt;rKB/s&lt;/code&gt; is uninteresting since it is simply following &lt;code&gt;r/s&lt;/code&gt; (because &lt;code&gt;avgrq-sz&lt;/code&gt; is constant). But note that we get around 140 MBps for sequential throughput. &lt;/p&gt;
&lt;p&gt;Now lets tackle &lt;code&gt;iops&lt;/code&gt;. We already explained the constant iops until bs=7. From bs=8, A couple of things are happening here. First note the &lt;code&gt;io&lt;/code&gt; column. For bs=8, we have read 11GB of data, so in the middle of the 60 second run, the entire file was page-cached in memory and the rest of time was served purely from ram. So the reduction in iops is because for some fraction of the 60 second time, we are actually bottlenecked on hard disk. For subsequent values of bs, the fraction of time of hard-disk bottleneck remains constant (and hence the fraction of time cpu bottlenecked also remains constant) but in the cpu bottleneck phase, since the block size has doubled, iops will have to halve. Note that in the initial stages values after bs=7. i.e., for bs=8, 9, 10 the decrease in iops is less than half. This is because the system_call/kernel overhead is still the majority of time. As block size gets bigger, The time copying from kernel space to user space overwhelms the systemCall/kernel time, and we observe proper halving of iops (see iops values bs=15). This also explains why &lt;code&gt;bw&lt;/code&gt; saturates after bs=15. It is caused because once kernel/systemCall time became negligible, the bandwidth is simply &lt;code&gt;iops*bs&lt;/code&gt;. This remains true for all subsequent values of bs.&lt;/p&gt;
&lt;p&gt;The story with &lt;code&gt;bw&lt;/code&gt; is similar. Up until bs=7, the bw has exactly doubled, because fio was 100% CPU bottlenecked. Starting from bs=8, there is a component that cannot be increased (when bottleneck is hard-disk) and there is a component that can be doubled (when bottleneck is cpu and 1microsecond system call processing time). So the average increase is 1.64 times previous value&lt;/p&gt;
&lt;h4&gt;Quantitative explanation&lt;/h4&gt;
&lt;p&gt;Assume the following numbers:
* Time to make one system call irrespective of block size = 1.1 microseconds
* RAM-to-RAM copy bandwidth 4.6 GBps (i.e., to copy from kernel space to user space)&lt;/p&gt;
&lt;p&gt;For bs=2^9, at the rate of 138 MBps disk read speed, the disk was busy for 3G/138MBps = 22.6 seconds. Number of fio io operations for the first 22.6 seconds= 3G/2^9 = 6291456.
For the next 60 - 22.6 = 37.4 seconds, requests were served purely from copying data from page cache to user space (RAM-to-RAM copy). Time for each io operation = 1.1 microseconds + 2^9 / 4.8GBps = 1.2 microseconds. So in 37.4 seconds, number of fio operations = 37.4 / 1.2 microseconds = 31.16 million fio operations. So net io operations = 31.16 million + 6291456 = 37458122. 
iops calculated over 60 seconds = 37458122/60 = 624302. This differs from actual value of 617759 by 1%.
The actual percentages of error using this method (starting from bs=512) is 
0.80, 3.64, 3.32, 9.78, 11.94, 13.33, 15.65, 15.38, 0.99, 10.13, 5.77, 3.85, 2.75, 3.17, 3.66, 12.44
Even with 15% error, its not too shabby i guess, in the face of other complications such as page faults and TLB misses&lt;/p&gt;
&lt;p&gt;Unexplained:
The &lt;code&gt;avgrq-sz&lt;/code&gt; seems to be stuck at 512 sectors. Why? we saw in the case of &lt;code&gt;libaio,randread,sync=0,direct=0&lt;/code&gt; that &lt;code&gt;avgrq-sz&lt;/code&gt; went upto 1024 sectors. &lt;/p&gt;
&lt;p&gt;explain the &lt;code&gt;bw&lt;/code&gt; from 2^16 onwards&lt;/p&gt;
&lt;p&gt;Quantitatively verify if the theories make sense. &lt;/p&gt;
&lt;h3&gt;sync,read,sync=1,direct=1&lt;/h3&gt;
&lt;p&gt;From bs=2^15 onwards, everything is straightforward. First, notice that iops follows r/s and bw follows rKB/s. This is expected for direct=1. So we can ignore those. 
The total time for an read operation = 1.1 microseconds (system call) + bs / (hdd bw = 130 MBps) + bs / (RAM bw = 4.8 BGps)&lt;/p&gt;
&lt;p&gt;The RAM bw is due to copying from kernel space to user space. &lt;/p&gt;
&lt;p&gt;The error percentages from computing r/s vs the actual r/s is as follows:
 5.72%, 5.19%, 6.72%, 5.22%, 5.25%, 5.68%, 6.52%, 8.25%, 11.86%, 19.85%&lt;/p&gt;
&lt;h5&gt;Unexplained&lt;/h5&gt;
&lt;p&gt;I am not able to explain what is going on for bs=2^{15, 16, 17}&lt;/p&gt;
&lt;h3&gt;sync,read,sync=0,direct={0,1}&lt;/h3&gt;
&lt;p&gt;This case is exactly the same as sync=1 because read operations are always sync=1. The sync=0 flag has no effect.&lt;/p&gt;
&lt;h3&gt;sync,randread,sync=0,direct=0&lt;/h3&gt;
&lt;p&gt;First, let us tackle the part of the graph until bs=2^19. Let's concentrate on r/s. In the beginning, the r/s is mainly influenced by the 5ms seek time for random read. This translates to ~170 reads per second. For later values of bs, starting from bs=2^15, the r/s starts dipping. What is happenning is that for bigger blocks, time taken to read a full block (at the rate of 130 MBps) starts dominating, which leads to a dip in r/s. &lt;/p&gt;
&lt;p&gt;Time taken for a read = 5ms (seek time) + bs / 110 MBps(hdd read throughput)&lt;/p&gt;
&lt;p&gt;The errors of computed vs actual r/s until bs=19 is as follows:
0.48%, 2.19%, 2.77%, 3.96%, 1.61%, 2.19%, 1.03%, 2.17%, 2.15%, 2.11%, 2.03%, 0.72%, 2.12%, 2.65%, 0.79%, 4.34%, 3.28%, -1.75%, 3.80%, 0.98%
As you can see, its remarkably accurate&lt;/p&gt;
&lt;p&gt;For rKB/s, the computed value is r/s * avgrq-sz
When we measure the difference between computed rKB/s and actual rKB/s, we get:
0.48%, 2.19%, 2.77%, 3.96%, 1.61%, 2.19%, 1.03%, 2.17%, 2.15%, 2.11%, 2.03%, 0.72%, 2.12%, 2.65%, 0.79%, 4.34%, 3.28%, -1.75%, 3.80%, 0.98%
The error rates are exactly the same as r/s which stresses that &lt;code&gt;rKB/s&lt;/code&gt; is a computed value rather than an independant value.&lt;/p&gt;
&lt;p&gt;For the bw graph. It is simply following &lt;code&gt;rKB/s&lt;/code&gt; until bs=19&lt;/p&gt;
&lt;p&gt;The iops graph is explained simply as &lt;code&gt;bw/bs&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;It is also easy to explain the graph from bs=20 onwards. Each fio block translates into bs/avrq-sq sequential disk read operations. For example, A fio block of size 2 MB results in 4 sequential reads (each of 1024 sectors = 0.5 MB). That explains the sudden increase in r/s after bs=19. 
If we now use hdd throughput = 120 MBps (retrofitted, but i've seen the throughput varying between 110-120MBps) with the formula &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;computed_read_s = (bs / (1024*512)) / (5.6*10**-3 + bs/(120*2**20) )
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The errors against actual values are as follows:
0.38%, 0.92%, -1.21%, -4.98%, -1.99%&lt;/p&gt;
&lt;p&gt;As always, the &lt;code&gt;rKB/s&lt;/code&gt; is r/s*avgrq-sz&lt;/p&gt;
&lt;p&gt;The iops and the bw graph is more interesting. Another thing that happened at bs=19 was that the page-cache was completely filled. (see the io column) This results in requests being served from RAM which explains the tremendous shootup in fio iops and fio bw&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;number of ios for reading the 3G file into memory = 3G/bs ------&amp;gt; (3)
time to read it from hard disk = 3G/rKBps
time for which requests were served from page-cache = 60-3G/rKBps   ----&amp;gt; (1)
Time of a single page-cache io operation = 1.1 microseconds + bs / (4.8 GBps = ram  bandwidth) ----&amp;gt; (2)
expected iops = ((3)+(1)/(2)) / 60
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;With the above formula, the errors of expected vs actual are 
10.11%, 1.39%, 2.22%, 3.41%, 9.97%&lt;/p&gt;
&lt;p&gt;The 10% variation is not unreasonable with the complicated L* cache hierarchies of modern processors&lt;/p&gt;
&lt;p&gt;The bw graph is simply iops * bs&lt;/p&gt;</content><category term="misc"></category><category term="misc"></category></entry><entry><title>C IDE with vim, cscope and grep (ag)</title><link href="/c-ide-with-vim-cscope-and-grep-ag.html" rel="alternate"></link><published>2016-05-02T22:20:00+00:00</published><updated>2016-05-02T22:20:00+00:00</updated><author><name>Sharath Gururaj</name></author><id>tag:None,2016-05-02:/c-ide-with-vim-cscope-and-grep-ag.html</id><summary type="html">&lt;h1&gt;Setup Cscope&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;First, Install cscope. &lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Next, you need a &lt;code&gt;cscope.files&lt;/code&gt; with all the files that need to be indexed. In the source folder, you can do that with  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;find&lt;/span&gt; &lt;span class="ss"&gt;`pwd`&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;*.c&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;o&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;*.cc&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;o&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;*.h&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;cscope&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;files&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;the &lt;code&gt;pwd&lt;/code&gt; is needed instead of &lt;code&gt;.&lt;/code&gt; because you need …&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;h1&gt;Setup Cscope&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;First, Install cscope. &lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Next, you need a &lt;code&gt;cscope.files&lt;/code&gt; with all the files that need to be indexed. In the source folder, you can do that with  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;find&lt;/span&gt; &lt;span class="ss"&gt;`pwd`&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;*.c&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;o&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;*.cc&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;o&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;*.h&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;cscope&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;files&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;the &lt;code&gt;pwd&lt;/code&gt; is needed instead of &lt;code&gt;.&lt;/code&gt; because you need &lt;code&gt;find&lt;/code&gt; to print absolute pathnames.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Now, build the cscope database.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;cscope -q -R -b -i cscope.files&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This command will build &lt;code&gt;cscope.out&lt;/code&gt;. &lt;code&gt;-q&lt;/code&gt; to build an efficient database, &lt;code&gt;-b&lt;/code&gt; to build just the database. &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Start up vim and issue the command&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;:cs add ~/code/ceph/cscope.out&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;which will add the cscope database. Vim has built in support for cscope in the form of the &lt;code&gt;:cs&lt;/code&gt; command. But it does &lt;em&gt;not&lt;/em&gt; have useful keybindings setup. For that, you need &lt;code&gt;cscopes_map.vim&lt;/code&gt; (which I've sourced in the &lt;code&gt;.vimrc&lt;/code&gt;). You're all set now. Some basic comands:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;C-\ s&lt;/code&gt; to find a symbol &lt;/li&gt;
&lt;li&gt;&lt;code&gt;C-\ c&lt;/code&gt; to find all callers&lt;/li&gt;
&lt;li&gt;&lt;code&gt;C-\ f&lt;/code&gt; to find global definitions&lt;/li&gt;
&lt;li&gt;&lt;code&gt;:help cscope&lt;/code&gt; for more&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;Setup ctags&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Install ctags.&lt;/li&gt;
&lt;li&gt;cd to the source folder and build the ctags index with &lt;code&gt;ctags -R .&lt;/code&gt; (&lt;code&gt;-R&lt;/code&gt; is for recursive).&lt;/li&gt;
&lt;li&gt;In vim, you need to do &lt;code&gt;:set tags=/path/to/tags&lt;/code&gt;. Unfortunately, you need to do this everytime you start vim. (how to avoid it?)&lt;/li&gt;
&lt;li&gt;use &lt;code&gt;C-]&lt;/code&gt; to show a list of tags under the cursor. &lt;code&gt;C-o&lt;/code&gt; or &lt;code&gt;C-t&lt;/code&gt; to go back.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;Some Vim shortcuts I would like to remember:&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;[[&lt;/code&gt; moves to the previous curly brace on column 0 (1?) &lt;/li&gt;
&lt;li&gt;&lt;code&gt;Ctrl-O&lt;/code&gt; and &lt;code&gt;Ctrl-I&lt;/code&gt; last location before jump&lt;/li&gt;
&lt;li&gt;&lt;code&gt;[{&lt;/code&gt; jump to begining of current block! (Or you can search backward for &lt;code&gt;{&lt;/code&gt;!&lt;/li&gt;
&lt;/ul&gt;</content><category term="misc"></category><category term="misc"></category></entry><entry><title>http and https proxy with squid</title><link href="/http-and-https-proxy-with-squid.html" rel="alternate"></link><published>2016-03-02T10:20:00+00:00</published><updated>2016-03-02T10:20:00+00:00</updated><author><name>Sharath Gururaj</name></author><id>tag:None,2016-03-02:/http-and-https-proxy-with-squid.html</id><summary type="html">&lt;h3&gt;On the client machine&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Find out the client subnet of the form &lt;code&gt;10.34.44.0/22&lt;/code&gt;. To do that, on the client machine, run &lt;code&gt;/sbin/ifconfig&lt;/code&gt; to find out the       ip address and subnet mask. with these two pieces of info, you can calculate the CIDR &lt;a href="http://www.subnet-calculator.com/cidr.php"&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;export variables …&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;h3&gt;On the client machine&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Find out the client subnet of the form &lt;code&gt;10.34.44.0/22&lt;/code&gt;. To do that, on the client machine, run &lt;code&gt;/sbin/ifconfig&lt;/code&gt; to find out the       ip address and subnet mask. with these two pieces of info, you can calculate the CIDR &lt;a href="http://www.subnet-calculator.com/cidr.php"&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;export variables: 
  &lt;code&gt;text
  export http_proxy=http://client.ip.ad.ress:port/
  export https_proxy=http://client.ip.ad.ress:port/&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Caveat&lt;/h3&gt;
&lt;p&gt;if you run sudo, make sure the variables are exported in root&lt;/p&gt;
&lt;h3&gt;On the proxy machine&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Install squid: &lt;code&gt;sudo apt-get install squid&lt;/code&gt;. &lt;/li&gt;
&lt;li&gt;
&lt;p&gt;change /etc/squid/squid.conf to add these lines
   &lt;code&gt;text
   acl wormhole src 10.34.44.0/22 # the client subnet who need the proxy
   http_access allow wormhole # wormhole is just a name
   icp_access allow wormhole&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;restart squid &lt;code&gt;sudo /etc/squid/squid.conf restart&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;And thats it! &lt;code&gt;curl -L www.google.com&lt;/code&gt; (&lt;code&gt;-L&lt;/code&gt; to follow redirects) will work.&lt;/p&gt;
&lt;p&gt;I needed this for &lt;code&gt;pip&lt;/code&gt;. With my setup, I can do &lt;code&gt;sudo export &amp;lt;blah&amp;gt;;export&amp;lt;blah&amp;gt;;pip install &amp;lt;package&amp;gt;&lt;/code&gt;. &lt;/p&gt;
&lt;p&gt;FYI, my squid version is&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;[sharath.g@d42-a-0003 /home/sharath.g]$ dpkg -s squid
Package: squid
Status: install ok installed
Priority: optional
Section: web
Installed-Size: 1864
Maintainer: Luigi Gangitano &amp;lt;luigi@debian.org&amp;gt;
Architecture: amd64
Version: 2.7.STABLE9-4.1+deb7u1
Replaces: squid-novm
Depends: libc6 (&amp;gt;= 2.7), libcomerr2 (&amp;gt;= 1.01), libdb5.1, libgssapi-krb5-2 (&amp;gt;= 1.10+dfsg~), libkrb5-3 (&amp;gt;= 1.6.dfsg.2), libldap-2.4-2 (&amp;gt;= 2.4.7), libpam0g (&amp;gt;= 0.99.7.1), netbase, adduser, logrotate (&amp;gt;= 3.5.4-1), squid-common (&amp;gt;= 2.7.STABLE9-4.1+deb7u1), lsb-base (&amp;gt;= 3.2-14), perl-modules
Pre-Depends: debconf (&amp;gt;= 1.2.9) | debconf-2.0
Suggests: squidclient, squid-cgi, logcheck-database, resolvconf (&amp;gt;= 0.40), smbclient, winbind
Conflicts: sarg (&amp;lt;&amp;lt; 1.1.1-2), squid-novm
Conffiles:
 /etc/init.d/squid 04af7c1f2d27c35db0200679fbc9bdbe
 /etc/logrotate.d/squid 0dd1fea0f842a58f538408754e747311
 /etc/resolvconf/update-libc.d/squid f8d0ffa84ddd982f32da05cb61bc479e
Description: Internet object cache (WWW proxy cache)
 This package provides the Squid Internet Object Cache developed by
 the National Laboratory for Applied Networking Research (NLANR) and
 Internet volunteers.
Homepage: http://www.squid-cache.org/
&lt;/pre&gt;&lt;/div&gt;</content><category term="misc"></category><category term="regex"></category></entry><entry><title>Regex recipes</title><link href="/regex-recipes.html" rel="alternate"></link><published>2016-02-21T10:20:00+00:00</published><updated>2016-02-21T10:20:00+00:00</updated><author><name>Sharath Gururaj</name></author><id>tag:None,2016-02-21:/regex-recipes.html</id><summary type="html">&lt;p&gt;I find myself always having to google around a lot for common regex recipes. Surprisingly, the most common cases cannot be easily found without extensive grokking.&lt;/p&gt;
&lt;h3&gt;Python examples&lt;/h3&gt;
&lt;h4&gt;Common special characters:&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;\d&lt;/code&gt;: only digit, &lt;/p&gt;
&lt;h4&gt;Search and replace&lt;/h4&gt;
&lt;p&gt;Search for occurences of a regex string and replace it with something that …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I find myself always having to google around a lot for common regex recipes. Surprisingly, the most common cases cannot be easily found without extensive grokking.&lt;/p&gt;
&lt;h3&gt;Python examples&lt;/h3&gt;
&lt;h4&gt;Common special characters:&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;\d&lt;/code&gt;: only digit, &lt;/p&gt;
&lt;h4&gt;Search and replace&lt;/h4&gt;
&lt;p&gt;Search for occurences of a regex string and replace it with something that depends on the actual value of the string.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;re&lt;/span&gt;
&lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;re&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;r&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;(blue|red)&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sub&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;group&lt;/span&gt;&lt;span class="p"&gt;())),&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;blue socks and red shoes&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;output: &lt;code&gt;4 socks and 3 shoes&lt;/code&gt;&lt;/p&gt;
&lt;h4&gt;Find all ip addresses in a text&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;p = re.compile(r&amp;quot;\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}[^0-9]&amp;quot;)
s = &amp;quot;Runbook:blah blah  IP:10.34.249.124 blah blah IP:10.33.157.166&amp;quot;
print p.findall(s)
&lt;/pre&gt;&lt;/div&gt;</content><category term="misc"></category><category term="regex"></category></entry><entry><title>Setting up a ceph cluster</title><link href="/setting-up-a-ceph-cluster.html" rel="alternate"></link><published>2016-02-07T10:20:00+00:00</published><updated>2016-02-07T10:20:00+00:00</updated><author><name>Sharath Gururaj</name></author><id>tag:None,2016-02-07:/setting-up-a-ceph-cluster.html</id><summary type="html">&lt;p&gt;The instructions &lt;a href="http://docs.ceph.com/docs/hammer/install/manual-deployment/"&gt;here&lt;/a&gt; mostly work. Except that the &lt;code&gt;ceph.conf&lt;/code&gt; file  given there doesnt work. Because, when I issue &lt;code&gt;sudo /etc/init.d/ceph start mon.node1&lt;/code&gt; in the end, It expects a section in &lt;code&gt;ceph.   conf&lt;/code&gt; called &lt;code&gt;[mon.node1]&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Furthermore, to launch a radosgw instance, you &lt;em&gt;need&lt;/em&gt; to have …&lt;/p&gt;</summary><content type="html">&lt;p&gt;The instructions &lt;a href="http://docs.ceph.com/docs/hammer/install/manual-deployment/"&gt;here&lt;/a&gt; mostly work. Except that the &lt;code&gt;ceph.conf&lt;/code&gt; file  given there doesnt work. Because, when I issue &lt;code&gt;sudo /etc/init.d/ceph start mon.node1&lt;/code&gt; in the end, It expects a section in &lt;code&gt;ceph.   conf&lt;/code&gt; called &lt;code&gt;[mon.node1]&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Furthermore, to launch a radosgw instance, you &lt;em&gt;need&lt;/em&gt; to have a section in your host file called &lt;code&gt;client.rgw.&amp;lt;some_name&amp;gt;&lt;/code&gt; and your  keyring should have a corresponding key section called  &lt;code&gt;client.rgw.&amp;lt;some_name&amp;gt;&lt;/code&gt; and you should have imported that key into ceph    via &lt;code&gt;sudo ceph auth import -i /etc/ceph/&amp;lt;keyring_file&amp;gt;&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;All the three names must match: the one in ceph.conf, in the keyring and in &lt;code&gt;ceph auth list&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Oh, and yeah, do &lt;strong&gt;NOT&lt;/strong&gt; put a line like this in your &lt;code&gt;ceph.conf&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;rgw &lt;span class="nv"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; /var/lib/ceph/radosgw/ceph-rgw.prod-d42sa-rgw-a-287004
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;If you do, you'l get weird messages like &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Couldn't init storage provider (RADOS)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;and you'll go crazy trying to debug it.&lt;/p&gt;
&lt;p&gt;So the working &lt;code&gt;ceph.conf&lt;/code&gt; with comments is: &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;global&lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="nv"&gt;fsid&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; a7f64266-0894-4f1e-a635-d0aeaca0e993 &lt;span class="c1"&gt;#anything random, generated from uuidgen&lt;/span&gt;
auth cluster &lt;span class="nv"&gt;required&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; cephx
auth service &lt;span class="nv"&gt;required&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; cephx
auth client &lt;span class="nv"&gt;required&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; cephx
osd journal &lt;span class="nv"&gt;size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;1024&lt;/span&gt;
filestore xattr use &lt;span class="nv"&gt;omap&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;true&lt;/span&gt;
osd pool default &lt;span class="nv"&gt;size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;2&lt;/span&gt; 
osd pool default min &lt;span class="nv"&gt;size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt; 
osd pool default pg &lt;span class="nv"&gt;num&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;333&lt;/span&gt; 
osd pool default pgp &lt;span class="nv"&gt;num&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;333&lt;/span&gt; 
osd crush chooseleaf &lt;span class="nb"&gt;type&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt; 
&lt;span class="c1"&gt;# useful for debugging&lt;/span&gt;
&lt;span class="o"&gt;[&lt;/span&gt;mon&lt;span class="o"&gt;]&lt;/span&gt;
  debug &lt;span class="nv"&gt;mon&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;20&lt;/span&gt;
&lt;span class="o"&gt;[&lt;/span&gt;osd&lt;span class="o"&gt;]&lt;/span&gt;
  debug &lt;span class="nv"&gt;osd&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;20&lt;/span&gt;
&lt;span class="o"&gt;[&lt;/span&gt;rgw&lt;span class="o"&gt;]&lt;/span&gt;
  debug &lt;span class="nv"&gt;rgw&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;20&lt;/span&gt;
&lt;span class="o"&gt;[&lt;/span&gt;mon.node1&lt;span class="o"&gt;]&lt;/span&gt; &lt;span class="c1"&gt;# as many sections as monitors&lt;/span&gt;
  &lt;span class="nv"&gt;host&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; dev-d42sharath1-mon-a-0001-389313 &lt;span class="c1"&gt;#This NEEDS to be the actual hostname donno for what reasons&lt;/span&gt;
  mon &lt;span class="nv"&gt;addr&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;10&lt;/span&gt;.33.29.199 &lt;span class="c1"&gt;# needs to be the actual address.&lt;/span&gt;
&lt;span class="o"&gt;[&lt;/span&gt;client.rgw.foo&lt;span class="o"&gt;]&lt;/span&gt;
    &lt;span class="nv"&gt;host&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; dev-d42sharath1-rgw-a-0001-389309
    &lt;span class="nv"&gt;keyring&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; /etc/ceph/ceph.client.rgw.keyring
    log &lt;span class="nv"&gt;file&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; /var/log/radosgw/client.radosgw.gateway.log
    rgw &lt;span class="nv"&gt;frontends&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; civetweb &lt;span class="nv"&gt;port&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;80&lt;/span&gt;
    rgw print &lt;span class="k"&gt;continue&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;false&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;</content><category term="misc"></category><category term="ceph"></category></entry><entry><title>I finally setup a blog</title><link href="/i-finally-setup-a-blog.html" rel="alternate"></link><published>2016-02-02T22:20:00+00:00</published><updated>2016-02-02T22:20:00+00:00</updated><author><name>Sharath Gururaj</name></author><id>tag:None,2016-02-02:/i-finally-setup-a-blog.html</id><summary type="html">&lt;p&gt;&lt;strong&gt;I finally setup my own blog!&lt;/strong&gt; and i'm happy I did it the right way. No lame wordpress. I bought a virtual private server on &lt;a href="www.digitalocean.com"&gt;digital ocean&lt;/a&gt;. &lt;/p&gt;
&lt;p&gt;Virtual private server (or a droplet in digitalocean lingo) is a fancy name for your own computer in the cloud, which you can …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;I finally setup my own blog!&lt;/strong&gt; and i'm happy I did it the right way. No lame wordpress. I bought a virtual private server on &lt;a href="www.digitalocean.com"&gt;digital ocean&lt;/a&gt;. &lt;/p&gt;
&lt;p&gt;Virtual private server (or a droplet in digitalocean lingo) is a fancy name for your own computer in the cloud, which you can ssh into and have   root access. It cost me ~Re 300 per month for their most basic droplet. I set it up with a static ipv4.&lt;/p&gt;
&lt;p&gt;Since digitalocean has no DNS service, I bought a domain on &lt;a href="www.namecheap.com"&gt;namecheap&lt;/a&gt; for Re 500 per year and pointed it to the droplet. I   am hosting this as a static site powered by pelican. Using the built-in python server for now.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;UPDATE (11th may 2016):&lt;/strong&gt; Paying monthly to digital ocean was getting to be a pain. So I have moved to github pages, which is just as simple.  &lt;/p&gt;</content><category term="misc"></category><category term="misc"></category></entry></feed>